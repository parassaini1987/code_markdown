{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d665db-88b3-45cf-8317-e21a403160e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Code Markdown with RAG + Gradio (Apple Mac Compatible Only)\n",
    "\n",
    "\"\"\"\n",
    "This notebook generates code markdown files using the CodeLlama GGUF model and allows querying explained content.\n",
    "It runs locally on Apple Silicon (M1/M2/M3 Macs)** using llama-cpp-python. GPU/CUDA not supported yet.\n",
    "\n",
    "Coming Soon: CUDA support for Linux/Windows users with NVIDIA GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "1. [Install Anaconda](https://www.anaconda.com/products/distribution#download-section)\n",
    "2. Create and activate a Conda environment:\n",
    "    ```bash\n",
    "    conda create -n code_llama_env python=3.10 -y\n",
    "    conda activate code_llama_env\n",
    "    ```\n",
    "3. [Create a Hugging Face token](https://huggingface.co/settings/tokens)\n",
    "    - Add it to a `.env` file in the same directory:\n",
    "      ```env\n",
    "      HF_TOKEN=your_token_here\n",
    "      ```\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Install Dependencies (only once)\n",
    "```bash\n",
    "pip install llama-cpp-python huggingface_hub chromadb sentence-transformers python-dotenv gradio\n",
    "```\n",
    "\n",
    "---\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102777ae-c4da-4ed8-8a4c-97a842f1af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only first time.\n",
    "!pip install huggingface_hub llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665af98-d00d-49a3-bf87-26da31cb3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usefull imports\n",
    "\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4992e-ddbd-42db-b551-e3f3dabfc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (.env must contain HF_TOKEN)\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "if HF_TOKEN:\n",
    "    print(f\"HF_TOKEN Key exists and begins {HF_TOKEN[:8]}\")\n",
    "else:\n",
    "    print(\"HF_TOKEN Key not set\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4710981-703f-4c5a-80cc-8a7d4d81e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CodeLlama GGUF Model (only if not already present)\n",
    "repo_id = \"TheBloke/CodeLlama-7B-Instruct-GGUF\"\n",
    "filename = \"codellama-7b-instruct.Q4_K_M.gguf\"\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename,\n",
    "        token=HF_TOKEN,\n",
    "        local_dir=model_dir\n",
    "    )\n",
    "    print(f\"‚úÖ Model downloaded to: {model_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model already exists at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc141ef-cc83-4292-b1ee-c81ec57ec988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CodeLlama model using llama-cpp-python\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,\n",
    "    n_threads=4  # Adjust based on your CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b0ca12-3d4a-4ac8-a011-e5aab503c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Input Code File\n",
    "def read_code_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "code = read_code_file(\"sample.cs\")  # Make sure this file exists in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ed07d-0603-4f7d-bbce-789451bcd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Prompt\n",
    "def build_prompt(code: str) -> str:\n",
    "    return f\"\"\"### Instruction:\n",
    "You are a helpful coding assistant. Explain what the following code does with detailed step-by-step explanation.\n",
    "Ignore if it is a binary file, image and compressed archive.\n",
    "Explain each class, functions and each line.\n",
    "At last put the conclusion at the end of file giving the brief summary.\n",
    "Do not include code in explaination.\n",
    "### Code:\n",
    "{code}\n",
    "\n",
    "### Explanation:\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165b005-f2b5-4e2f-9239-be554a4df84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate code markdown using LLM.\n",
    "\n",
    "prompt = build_prompt(code)\n",
    "\n",
    "output = llm(prompt, max_tokens=512)\n",
    "explanation = output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "\n",
    "\n",
    "print(\"üîç Explanation:\\n\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76423034-a5d6-43cd-b20a-6ec93b0fd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated markdown to File\n",
    "with open(\"code_markdown.txt\", \"w\") as f:\n",
    "    f.write(explanation)\n",
    "\n",
    "print(\"Markdown saved to 'code_markdown.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b569c-00e4-444b-8d4c-13abee47a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedder and Vector DB\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(\"code_explanations\")\n",
    "code_path = \"sample.cs\"\n",
    "embedding = embedder.encode(explanation)\n",
    "collection.add(\n",
    "    documents=[explanation],\n",
    "    metadatas=[{\"source\": code}],\n",
    "    ids=[f\"code-{os.path.basename(code_path)}\"],\n",
    "    embeddings=[embedding]\n",
    ")\n",
    "print(\"Stored in ChromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85a584-9795-4228-ac83-dd1fdf62cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"How many nested loops are there.?\"  # Replace with your question\n",
    "question_embedding = embedder.encode(question)\n",
    "results = collection.query(query_embeddings=[question_embedding], n_results=1)\n",
    "context = results[\"documents\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678326e-883b-4b1d-bc8b-7219de0fbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe77c96-92e8-4a54-9050-9d7101eb9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = f\"\"\"### Instruction:\n",
    "Use the context below to answer the user question.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "answer_response = llm(qa_prompt, max_tokens=256)\n",
    "answer = answer_response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "print(\"Answer:\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
